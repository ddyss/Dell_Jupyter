{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.io\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd1=torch.randn(2,32,128,128) \n",
    "dd1=dd1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_score(nn.Module):\n",
    "    def __init__(self, num_channel, hidden_size):\n",
    "        super(CNN_score, self).__init__()\n",
    "        self.num_channel = num_channel\n",
    "        self.hidden_size = hidden_size\n",
    "        # +: layer1->16\n",
    "        # ++: layer1, layer0->16\n",
    "        self.layer0 = nn.Conv1d(num_channel, 1, kernel_size=9, padding=4) #9 4\n",
    "        nn.init.xavier_uniform(self.layer0.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "      \n",
    "        self.layer1 = nn.Conv2d(2, 1, kernel_size=(num_channel,9), padding=(0,4), stride=(num_channel,1)) #\n",
    "        nn.init.xavier_uniform(self.layer1.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "\n",
    "        self.fc00 = nn.Linear(hidden_size, num_channel) # \n",
    "        nn.init.xavier_uniform(self.fc00.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.fc01 = nn.Linear(hidden_size, num_channel) #\n",
    "        nn.init.xavier_uniform(self.fc01.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, num_channel) # \n",
    "        nn.init.xavier_uniform(self.fc1.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.fc2 = nn.Linear(num_channel, num_channel) # \n",
    "        nn.init.xavier_uniform(self.fc2.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, h_i, pre_h_i, pre_s):\n",
    "        # shape: batch x channel x hidden_size\n",
    "        # shape: batch x channel x hidden_size  added by myself\n",
    "        # shape: batch x channel\n",
    "        \n",
    "        out_h_i = self.layer0(h_i) # batch x 1 x hidden_size  把channel变成了1\n",
    "        out_h_i = self.fc00(out_h_i.view(out_h_i.size(0),-1)) # batch x channel\n",
    "        out_pre_h_i = self.layer0(pre_h_i) # batch x 1 x something124\n",
    "        out_pre_h_i = self.fc01(out_pre_h_i.view(out_pre_h_i.size(0),-1)) # batch x channel\n",
    "        \n",
    "        \n",
    "        h_i = h_i.view(h_i.size(0),-1,h_i.size(1),h_i.size(2)) # batch x 1 x channel x hidden_size\n",
    "        pre_h_i = pre_h_i.view(pre_h_i.size(0),-1,pre_h_i.size(1),pre_h_i.size(2)) # batch x 1 x channel x hidden_size\n",
    "        hh = torch.cat((h_i,pre_h_i), 1) # batch x 2 x channel x hidden_size\n",
    "        \n",
    "        out_hh = self.layer1(hh) # batch x 1 x hidden_size 又把channel变成了1\n",
    "        out_hh = self.fc1(out_hh.view(out_hh.size(0),-1)) # batch x channel\n",
    "        pre_s = self.fc2(pre_s)\n",
    "        \n",
    "        out = self.tanh(out_h_i + out_pre_h_i + out_hh + pre_s)\n",
    "        return out  #batch x channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_channel, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "\n",
    "        self.attention_layer = CNN_score(num_channel, hidden_size)\n",
    "            \n",
    "    def forward(self, h):\n",
    "        # shape: batch x seq x channel x hidden_size\n",
    "        batch_size = h.size(0)\n",
    "        seq_size = h.size(1)\n",
    "        num_channel = h.size(2)\n",
    "\n",
    "        context_matrix = to_var(torch.zeros((batch_size, seq_size, num_channel, self.hidden_size))) # batch x seq x channel x hidden_size\n",
    "        \n",
    "        for i in range(h.size(1)): #seq_size\n",
    "            hh_i = h[:,i,:,:] # current hidden state: batch x channel x hidden_size\n",
    "            if i == 0:\n",
    "                scores = to_var(torch.zeros((batch_size, 1, num_channel)))\n",
    "                pre_hh_i = h[:, i, :, :] * 0.0 # batch x channel x hidden_size\n",
    "            else:\n",
    "                scores = to_var(torch.zeros((batch_size, i, num_channel))) # batch x sub_seq_size x channel\n",
    "                pre_hh_i = h[:, :i, :, :] # previous hidden states: batch x sub_seq_size x channel x hidden_size\n",
    "                for j in range(pre_hh_i.size(1)): #sub_seq_size\n",
    "                    if j == 0:\n",
    "                        scores[:,j,:] = self.energy(hh_i, pre_hh_i[:,j,:,:], to_var(torch.zeros((batch_size, num_channel))))\n",
    "                    else:\n",
    "                        pre_score = scores[:,j-1,:].clone() #克隆但不改变本体\n",
    "                        scores[:,j,:] = self.energy(hh_i, pre_hh_i[:,j,:,:], pre_score) # batch x channel\n",
    "            \n",
    "            \n",
    "            scores = self.normalization(scores, 2) # batch x sub_seq_size x channel\n",
    "\n",
    "            scores = scores.view(scores.size(0),scores.size(1),scores.size(2),-1) # batch x sub_seq_size x channel x 1\n",
    "            scores = scores.expand(scores.size(0),scores.size(1),scores.size(2),self.hidden_size) # batch x sub_seq_size x channel x hidden_size\n",
    "            \n",
    "            context = pre_hh_i * scores # batch x sub_seq_size x channel x hidden_size\n",
    "            context = context.sum(1) # batch x channel x hidden_size\n",
    "            context_matrix[:,i,:,:] = context # batch x 1 x channel x hidden_size\n",
    "            \n",
    "        # batch x seq x 1 x channel x hidden_size\n",
    "        context_matrix = context_matrix.view(context_matrix.size(0),context_matrix.size(1),-1,context_matrix.size(2),context_matrix.size(3))\n",
    "        h = h.view(h.size(0),h.size(1),-1,h.size(2),h.size(3))\n",
    "        out = torch.cat([context_matrix,h],2) # batch x seq x 2 x channel x hidden_size\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def energy(self, hidden_i, pre_hidden_i, pre_scores): \n",
    "        # shape: batch x channel x hidden_size\n",
    "        # shape: batch x channel\n",
    "\n",
    "        energies = to_var(torch.zeros((hidden_i.size(0), hidden_i.size(1)))) # batch x channel\n",
    "        h_i = hidden_i.clone()\n",
    "        pre_h_i = pre_hidden_i.contiguous()\n",
    "        energies = self.attention_layer(h_i, pre_h_i, pre_scores)  #放CNN_score中\n",
    "        \n",
    "        return energies  #batch x channel\n",
    "        \n",
    "    def normalization(self, scores, gamma):\n",
    "        # shape: batch x sub_seq_size x channel\n",
    "        sub_seq_size = scores.size(1) \n",
    "        num_channel = scores.size(2)\n",
    "        gamma_d = self.relu(scores).sum(2) # batch x sub_seq_size\n",
    "        gamma_d_sum = gamma_d.sum(1,keepdim=True) + 1e-8 # batch x 1\n",
    "        gamma_d_sum = gamma_d_sum.expand(gamma_d_sum.size(0),sub_seq_size) # batch x sub_seq_size\n",
    "        gamma_d = gamma_d / gamma_d_sum # batch x sub_seq_size\n",
    "        gamma_d = gamma_d.view(gamma_d.size(0),gamma_d.size(1),-1) # batch x sub_seq_size x 1\n",
    "        gamma_d =  gamma_d.expand(gamma_d.size(0),gamma_d.size(1),num_channel) # batch x sub_seq_size x channel\n",
    "        \n",
    "        \n",
    "        scores = self.sigmoid(scores)\n",
    "        out = to_var(torch.zeros(scores.size(0),sub_seq_size,num_channel)) # batch x sub_seq_size x channel\n",
    "        for i in range(sub_seq_size):\n",
    "            scores_i_sum = scores[:,i,:].sum(1,keepdim=True) + 1e-8 # batch x 1\n",
    "            scores_i_sum = scores_i_sum.expand(scores_i_sum.size(0),num_channel) # batch x channel\n",
    "            out[:,i,:] = scores[:,i,:] / scores_i_sum        \n",
    "           \n",
    "        out  = gamma_d * out # batch x sub_seq_size x channel\n",
    "        out = out.view(out.size(0),-1) # batch x (sub_seq_size * channel)\n",
    "        out = F.softmax(gamma * out) # batch x (sub_seq_size * channel) \n",
    "        out = out.view(out.size(0), sub_seq_size, num_channel) # batch x sub_seq_size x channel\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if __name__ == '__main__':\n",
      "e:\\python\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if sys.path[0] == '':\n",
      "e:\\python\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n",
      "e:\\python\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "e:\\python\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "e:\\python\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "djgnet=Attention(num_channel=128,hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\lib\\site-packages\\ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "2\n",
      "torch.Size([2, 32, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "djgnet=djgnet.to('cuda')\n",
    "yy=djgnet(dd1)\n",
    "print(type(yy))\n",
    "print(len(yy))\n",
    "print(yy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
